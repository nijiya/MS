
@inproceedings{simsek_using_2004,
	address = {New York, {NY}, {USA}},
	series = {{ICML} '04},
	title = {Using relative novelty to identify useful temporal abstractions in reinforcement learning},
	isbn = {1-58113-838-5},
	location = {Banff, Alberta, Canada},
	doi = {http://doi.acm.org/10.1145/1015330.1015353},
	publisher = {{ACM}},
	author = {\c{S}im\c{s}ek, \"{O}zg\"{u}r and Barto, Andrew G.},
	year = {2004},
	pages = {95--102},
	file = {Using Relative Novelty to Identify Useful Temporal Abstractions in Reinforcement Learning.pdf:/home/erkin/.mozilla/firefox/y7u6nwut.default/zotero/storage/A5DAEWGA/Using Relative Novelty to Identify Useful Temporal Abstractions in Reinforcement Learning.pdf:application/pdf}
},

@book{sigaud_markov_2010,
	title = {Markov Decision Processes in Artificial Intelligence},
	isbn = {1848211678, 9781848211674},
	publisher = {Wiley-{IEEE} Press},
	author = {Sigaud, Olivier and Buffet, Olivier},
	year = {2010},
	keywords = {***}
},

@inproceedings{charlin_automated_2007,
	address = {Cambridge, {MA}, {USA}},
	title = {Automated hierarchy discovery for planning in partially observable environments},
	volume = {19},
	publisher = {{MIT} Press},
	author = {Charlin, Laurent and Poupart, Pascal and Shioda, Romy},
	editor = {Sch\"{o}lkopf, B. and Platt, J. and Hoffman, T.},
	year = {2007},
	keywords = {**},
	pages = {225--232},
	file = {NIPS2006_0842.pdf:/home/erkin/.mozilla/firefox/y7u6nwut.default/zotero/storage/XU2TCPKM/NIPS2006_0842.pdf:application/pdf}
},

@inproceedings{mcgovern_acquire-macros:_1998,
	title = {{acQuire-macros:} An algorithm for automatically learning macro-actions},
	booktitle = {Neural Information Processing Systems Conference workshop on Abstraction and Hierarchy in Reinforcement Learning},
	author = {{McGovern}, Amy},
	year = {1998},
	file = {acQuire-macros An Algorithm for Automatically Learning Macro-actions.pdf:/home/erkin/.mozilla/firefox/y7u6nwut.default/zotero/storage/GTRTJM2C/acQuire-macros An Algorithm for Automatically Learning Macro-actions.pdf:application/pdf}
},

@article{girgin_improving_2010,
	title = {Improving reinforcement learning by using sequence trees},
	volume = {81},
	issn = {0885-6125},
	number = {3},
	journal = {Machine Learning},
	author = {Girgin, Sertan and Polat, Faruk and Alhajj, Reda},
	year = {2010},
	pages = {283--331},
	file = {fulltext.pdf:/home/erkin/.mozilla/firefox/y7u6nwut.default/zotero/storage/KTRWCT9A/fulltext.pdf:application/pdf}
},

@article{sutton_between_1999,
	title = {Between {MDPs} and semi-{MDPs:} a framework for temporal abstraction in reinforcement learning},
	volume = {112},
	issn = {0004-3702},
	doi = {http://dx.doi.org/10.1016/S0004-3702(99)00052-1},
	number = {1-2},
	journal = {Artificial Intelligence},
	author = {Sutton, Richard S. and Precup, Doina and Singh, Satinder},
	year = {1999},
	pages = {181--211},
	file = {Between MDPs and Semi-MDPs A Framework for Temporal Abstraction in Reinforcement Learning.pdf:/home/erkin/.mozilla/firefox/y7u6nwut.default/zotero/storage/FZITXI2C/Between MDPs and Semi-MDPs A Framework for Temporal Abstraction in Reinforcement Learning.pdf:application/pdf}
},

@incollection{theocharous_approximate_2004,
	address = {Vancouver and Whistler, British Columbia, Canada},
	series = {{NIPS}},
	title = {Approximate planning in {POMDPs} with macro-actions},
	booktitle = {Advances in Neural Processing Information Systems 16},
	author = {Theocharous, Georgios and Kaelbling, Leslie Pack},
	editor = {Sebastian Thrun and Lawrence K. Saul and Bernhard Sch\"{o}lkopf},
	year = {2004},
	file = {download.pdf:/home/erkin/.mozilla/firefox/y7u6nwut.default/zotero/storage/RQJRV9UA/download.pdf:application/pdf}
},

@inproceedings{bradtke_reinforcement_1994,
	title = {Reinforcement learning methods for continuous-time markov decision problems},
	volume = {7},
	booktitle = {Advances in Neural Information Processing Systems},
	publisher = {The {MIT} Press},
	author = {Bradtke, Steven J. and Duff, Michael O.},
	year = {1994},
	pages = {393--400},
	file = {10.1.1.55.4868_20101221_140358.pdf:/home/erkin/.mozilla/firefox/y7u6nwut.default/zotero/storage/IE5V3NIV/10.1.1.55.4868_20101221_140358.pdf:application/pdf}
},

@inproceedings{mcgovern_automatic_2001,
	address = {San Francisco, {CA}, {USA}},
	series = {{ICML'01}},
	title = {Automatic discovery of subgoals in reinforcement learning using diverse density},
	isbn = {1-55860-778-1},
	booktitle = {Proceedings of the Eighteenth International Conference on Machine Learning},
	publisher = {Morgan Kaufmann Publishers Inc.},
	author = {{McGovern}, Amy and Barto, Andrew G.},
	year = {2001},
	pages = {361--368},
	file = {Automatic Discovery of Subgoals in Reinforcement Learning using Diverse Density.pdf:/home/erkin/.mozilla/firefox/y7u6nwut.default/zotero/storage/STVCQID9/Automatic Discovery of Subgoals in Reinforcement Learning using Diverse Density.pdf:application/pdf}
},

@inproceedings{hengst_discovering_2002,
	address = {San Francisco, {CA}, {USA}},
	series = {{ICML'02}},
	title = {Discovering hierarchy in reinforcement learning with {HEXQ}},
	isbn = {1-55860-873-7},
	booktitle = {Proceedings of the Nineteenth International Conference on Machine Learning},
	publisher = {Morgan Kaufmann Publishers Inc.},
	author = {Hengst, Bernhard},
	year = {2002},
	pages = {243--250},
	file = {10.1.1.9.5839 (09-42-53).pdf:/home/erkin/.mozilla/firefox/y7u6nwut.default/zotero/storage/69JE9GIC/10.1.1.9.5839 (09-42-53).pdf:application/pdf}
},

@phdthesis{mcgovern_autonomous_2002,
	address = {Amherst, {MA}, {USA}},
	title = {Autonomous Discovery of Temporal Abstractions from Interaction with an Environment},
	school = {University of Massachusetts Amherst},
	author = {{McGovern}, Elizabeth Amy},
	year = {2002},
	file = {Autonomous Discovery of Temporal Abstractions from Interaction with an Environment.pdf:/home/erkin/.mozilla/firefox/y7u6nwut.default/zotero/storage/QTP7CXM4/Autonomous Discovery of Temporal Abstractions from Interaction with an Environment.pdf:application/pdf}
},

@article{barto_recent_2003,
	title = {Recent advances in hierarchical reinforcement learning},
	volume = {13},
	issn = {0924-6703},
	doi = {10.1023/A:1025696116075},
	number = {4},
	journal = {Discrete Event Dynamic Systems},
	author = {Barto, Andrew G. and Mahadevan, Sridhar},
	month = oct,
	year = {2003},
	keywords = {hierarchy, Markov decision processes, Reinforcement learning, Semi-Markov decision processes, temporal abstraction},
	pages = {341--379},
	file = {Recent Advances in Hierarchical Reinforcement Learning.pdf:/home/erkin/.mozilla/firefox/y7u6nwut.default/zotero/storage/QT34VETD/Recent Advances in Hierarchical Reinforcement Learning.pdf:application/pdf}
},

@article{kaelbling_planning_1998,
	title = {Planning and acting in partially observable stochastic domains},
	volume = {101},
	issn = {0004-3702},
	doi = {10.1016/S0004-3702(98)00023-X},
	number = {1-2},
	journal = {Artificial Intelligence},
	author = {Kaelbling, Leslie Pack and Littman, Michael L. and Cassandra, Anthony R.},
	month = may,
	year = {1998},
	keywords = {unread},
	pages = {99--134},
	file = {10.1.1.107.9127 (16-43-27).pdf:/home/erkin/.mozilla/firefox/y7u6nwut.default/zotero/storage/5PZC8ZC9/10.1.1.107.9127 (16-43-27).pdf:application/pdf}
},

@phdthesis{watkins_learning_1989,
	address = {England},
	title = {Learning from Delayed Rewards},
	school = {University of Cambridge},
	author = {Watkins, Chris},
	year = {1989},
	file = {new_thesis_20101222_191031.pdf:/home/erkin/.mozilla/firefox/y7u6nwut.default/zotero/storage/82K3QTT3/new_thesis_20101222_191031.pdf:application/pdf}
},

@inproceedings{chrisman_reinforcement_1992,
	series = {{AAAI'92}},
	title = {Reinforcement learning with perceptual aliasing: the perceptual distinctions approach},
	isbn = {0-262-51063-4},
	location = {San Jose, California},
	booktitle = {Proceedings of the Tenth National Conference on Artificial Intelligence},
	publisher = {{AAAI} Press},
	author = {Chrisman, Lonnie},
	year = {1992},
	pages = {183--188},
	file = {10.1.1.56.7115.pdf:/home/erkin/.mozilla/firefox/y7u6nwut.default/zotero/storage/PCRDMANI/10.1.1.56.7115.pdf:application/pdf}
},

@incollection{littman_learning_1998,
	address = {San Francisco, {CA}, {USA}},
	title = {Learning policies for partially observable environments: Scaling Up},
	isbn = {1-55860-495-2},
	booktitle = {Readings in Agents},
	publisher = {Morgan Kaufmann Publishers Inc.},
	author = {Littman, Michael L. and Cassandra, Anthony R. and Kaelbling, Leslie Pack},
	editor = {Huhns, Michael N. and Singh, Munindar P.},
	year = {1998},
	keywords = {***},
	pages = {495--503},
	file = {ml95.ps:/home/erkin/.mozilla/firefox/y7u6nwut.default/zotero/storage/FD6AX7X5/ml95.ps:application/postscript}
},

@incollection{stolle_learning_2002,
	address = {London, {UK}},
	series = {Lecture Notes in Computer Science},
	title = {Learning options in reinforcement learning},
	volume = {2371},
	isbn = {978-3-540-43941-7},
	booktitle = {Abstraction, Reformulation, and Approximation},
	publisher = {Springer Berlin / Heidelberg},
	author = {Stolle, Martin and Precup, Doina},
	year = {2002},
	pages = {212--223},
	file = {Learning Options in Reinforcement Learning.pdf:/home/erkin/.mozilla/firefox/y7u6nwut.default/zotero/storage/KR66EUS3/Learning Options in Reinforcement Learning.pdf:application/pdf}
},

@book{sutton_reinforcement_1998,
	series = {Adaptive Computation and Machine Learning},
	title = {Reinforcement Learning: An Introduction},
	isbn = {9780262193986},
	lccn = {97026416},
	publisher = {{MIT} Press},
	author = {Sutton, Richard S. and Barto, Andrew G.},
	year = {1998},
	file = {Sutton R.S.Reinforcement learning.An introduction.2005.chm:/home/erkin/.mozilla/firefox/y7u6nwut.default/zotero/storage/XT9RARXC/Sutton R.S.Reinforcement learning.An introduction.2005.chm:chemical/x-chemdraw}
},

@book{bellman_dynamic_1957,
	address = {Princeton, {NJ}},
	title = {Dynamic Programming},
	publisher = {Princeton University Press},
	author = {Bellman, Richard Ernest},
	year = {1957},
	file = {Dynamic Programming (R. Bellman) [PRINCETON, 1957].djvu:/home/erkin/.mozilla/firefox/y7u6nwut.default/zotero/storage/FIZSG859/Dynamic Programming (R. Bellman) [PRINCETON, 1957].djvu:image/vnd.djvu}
},

@phdthesis{parr_hierarchical_1998,
	address = {Berkeley, {CA}, {USA}},
	title = {Hierarchical Control and Learning for Markov Decision Processes},
	school = {University of California},
	author = {Parr, Ronald Edward},
	year = {1998},
	file = {Hierarchical Control and Learning for Markov Decision Processes.pdf:/home/erkin/.mozilla/firefox/y7u6nwut.default/zotero/storage/PIEZHT7I/Hierarchical Control and Learning for Markov Decision Processes.pdf:application/pdf}
},

@inproceedings{dung_reinforcement_2007,
	title = {Reinforcement learning in non-markovian environments using automatic discovery of subgoals},
	doi = {10.1109/SICE.2007.4421430},
	booktitle = {{SICE}, 2007 Annual Conference},
	author = {Dung, Le Tien and Komeda, Takashi and Takagi, Motoki},
	month = sep,
	year = {2007},
	keywords = {(artificial, automatic, discovery;virtual, E, environments;recurrent, intelligence);prediction, learning;subgoal, maze, nets;, networks;reinforcement, neural, office, {prediction;nonMarkovian}, problem;learning, {problem;Q}, theory;recurrent, values},
	pages = {2601--2605},
	file = {04421430.pdf:/home/erkin/.mozilla/firefox/y7u6nwut.default/zotero/storage/963UG8AD/04421430.pdf:application/pdf}
},

@article{dietterich_hierarchical_2000,
	title = {Hierarchical reinforcement learning with the {MAXQ} value function decomposition},
	volume = {13},
	issn = {1076-9757},
	journal = {Journal of Artificial Intelligence Research},
	author = {Dietterich, Thomas G.},
	year = {2000},
	keywords = {imported},
	pages = {227--303},
	file = {Hierarchical Reinforcement Learning with the MAXQ Value Function Decomposition.pdf:/home/erkin/.mozilla/firefox/y7u6nwut.default/zotero/storage/7XPIX9UT/Hierarchical Reinforcement Learning with the MAXQ Value Function Decomposition.pdf:application/pdf}
},

@inproceedings{menache_q-cut_2002,
	address = {London, {UK}},
	series = {{ECML'02}},
	title = {Q-Cut - Dynamic discovery of sub-goals in reinforcement learning},
	isbn = {3-540-44036-4},
	booktitle = {Proceedings of the 13th European Conference on Machine Learning},
	publisher = {Springer-Verlag},
	author = {Menache, Ishai and Mannor, Shie and Shimkin, Nahum},
	year = {2002},
	pages = {295--306},
	file = {10.1.1.8.255 (09-53-33).pdf:/home/erkin/.mozilla/firefox/y7u6nwut.default/zotero/storage/QW5C5DR3/10.1.1.8.255 (09-53-33).pdf:application/pdf}
},

@article{kaelbling_reinforcement_1996,
	title = {Reinforcement learning: a survey},
	volume = {4},
	journal = {Journal of Artificial Intelligence Research},
	author = {Kaelbling, Leslie Pack and Littman, Michael L. and Moore, Andrew P.},
	year = {1996},
	pages = {237--285},
	file = {Reinforcement Learning A Survey.pdf:/home/erkin/.mozilla/firefox/y7u6nwut.default/zotero/storage/3D2WN2QR/Reinforcement Learning A Survey.pdf:application/pdf}
},

@article{sutton_learning_1988,
	title = {Learning to predict by the methods of temporal differences},
	volume = {3},
	issn = {0885-6125},
	doi = {10.1023/A:1022633531479},
	number = {1},
	journal = {Machine Learning},
	author = {Sutton, Richard S.},
	month = aug,
	year = {1988},
	pages = {9--44},
	file = {download.pdf:/home/erkin/.mozilla/firefox/y7u6nwut.default/zotero/storage/TKVR9BA7/download.pdf:application/pdf}
},

@inproceedings{mannor_dynamic_2004,
	address = {New York, {NY}, {USA}},
	series = {{ICML'04}},
	title = {Dynamic abstraction in reinforcement learning via clustering},
	isbn = {1-58113-838-5},
	location = {Banff, Alberta, Canada},
	doi = {http://doi.acm.org/10.1145/1015330.1015355},
	publisher = {{ACM}},
	author = {Mannor, Shie and Menache, Ishai and Hoze, Amit and Klein, Uri},
	year = {2004},
	keywords = {Clustering, Hierarchical Reinforcement Learning, Options, Q-Learning, Reinforcement learning},
	pages = {71--78},
	file = {Dynamic Abstraction in Reinforcement Learning via Clustering.pdf:/home/erkin/.mozilla/firefox/y7u6nwut.default/zotero/storage/PXVVT572/Dynamic Abstraction in Reinforcement Learning via Clustering.pdf:application/pdf}
},

@inproceedings{smith_heuristic_2004,
	address = {Banff, Canada},
	series = {{UAI'04}},
	title = {Heuristic search value iteration for {POMDPs}},
	isbn = {0-9749039-0-6},
	location = {Banff, Canada},
	booktitle = {Proceedings of the 20th conference on Uncertainty in artificial intelligence},
	publisher = {{AUAI} Press},
	author = {Smith, Trey and Simmons, Reid},
	year = {2004},
	pages = {520--527},
	file = {ft_gateway.pdf:/home/erkin/.mozilla/firefox/y7u6nwut.default/zotero/storage/VZXUDBJT/ft_gateway.pdf:application/pdf}
}
